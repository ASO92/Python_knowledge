{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from datetime import datetime, timedelta, date\n",
    "import os\n",
    "\n",
    "import housekeeper\n",
    "\n",
    "class DataManager(ABC):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.__myHousekeeper = housekeeper.instance_class()\n",
    "        self.__config_filename = \"tickers_config.json\"\n",
    "        self.__dir_list = ['Data', 'Tickers', 'Dummy1']\n",
    "        self.__upper_stages = 0\n",
    "        self.__tickers_config_list = []\n",
    "        self.__tickers_list = []\n",
    "        self.__active_tickers_list = []\n",
    "        self.__selected_tickers_list = []\n",
    "        self.__timestamp = ''\n",
    "        self.__markets = []\n",
    "        self.__last_date_flag = False\n",
    "    \n",
    "\n",
    "    def get_config_filename(self):\n",
    "        return self.__config_filename\n",
    "    \n",
    "    def set_config_filename(self, config_filename):\n",
    "        self.__config_filename = config_filename\n",
    "        \n",
    "    def get_dir_list(self):\n",
    "        return self.__dir_list\n",
    "    \n",
    "    def set_dir_list(self, dir_list):\n",
    "        self.__dir_list = dir_list\n",
    "    \n",
    "    def get_upper_stages(self):\n",
    "        return self.__upper_stages\n",
    "    \n",
    "    def set_upper_stages(self, upper_stages):\n",
    "        self.__upper_stages = dir_list\n",
    "        \n",
    "    def get_last_date_flag(self):\n",
    "        return self.__last_date_flag\n",
    "    \n",
    "    def set_last_date_flag(self, last_date_flag):\n",
    "        self.__last_date_flag = last_date_flag\n",
    "        \n",
    "    def get_tickers_config(self):\n",
    "        return self.__tickers_config_list\n",
    "    \n",
    "    def set_tickers_config(self, tickers_config_list):\n",
    "        self.__tickers_config_list = tickers_config_list\n",
    "    \n",
    "    def get_tickers(self):\n",
    "        return self.__tickers_list\n",
    "    \n",
    "    def set_tickers(self, tickers_list):\n",
    "        self.__tickers_list = tickers_list\n",
    "        \n",
    "    def get_active_tickers(self):\n",
    "        return self.__active_tickers_list\n",
    "    \n",
    "    def set_active_tickers(self, active_tickers_list):\n",
    "        self.__active_tickers_list = active_tickers_list\n",
    "        \n",
    "    def get_selected_tickers(self):\n",
    "        return self.__selected_tickers_list\n",
    "    \n",
    "    def set_selected_tickers(self, selected_tickers_list):\n",
    "        self.__selected_tickers_list = selected_tickers_list\n",
    "    \n",
    "    def get_timestamp(self):\n",
    "        return self.__timestamp\n",
    "    \n",
    "    def set_timestamp(self, timestamp):\n",
    "        self.__timestamp = timestamp\n",
    "    \n",
    "    def get_markets(self):\n",
    "        return self.__markets\n",
    "    \n",
    "    def set_markets(self, markets):\n",
    "        self.__markets = markets\n",
    "    \n",
    "    def load_tickers_config(self):\n",
    "        data = self.__myHousekeeper.load_json_to_list(self.__dir_list, self.__config_filename)\n",
    "        self.set_tickers_config(data)\n",
    "        \n",
    "    def save_tickers_config(self):\n",
    "        #No invocar a esta función sin previamente haber cargado tickers_config. O se sobreescribe tickers_config\n",
    "        tickers_config = self.get_tickers_config()\n",
    "        self.__myHousekeeper.list_dict_to_json(self.get_dir_list(), \n",
    "                                               self.get_upper_stages(), \n",
    "                                               self.get_config_filename(), \n",
    "                                               self.get_tickers_config())\n",
    "    \n",
    "    def initialize_metadata(self):\n",
    "        self.load_tickers_config()\n",
    "        data = self.get_tickers_config()\n",
    "        self.set_timestamp(data['metadata'][0]['timestamp'])\n",
    "        self.set_tickers(data['data'])\n",
    "        \n",
    "    def initialize_config_tickers(self):\n",
    "        # Get markets, get active_tickers\n",
    "        markets = []\n",
    "        active_tickers_ = []\n",
    "        self.initialize_metadata()\n",
    "        data = self.get_tickers()\n",
    "        for d in data:\n",
    "            markets.append(d['market'])\n",
    "            if d['active_type']=='stock' and d['active_flag']:\n",
    "                active_tickers_.append(d)\n",
    "            elif d['active_type']=='ETF':\n",
    "                active_tickers_.append(d)\n",
    "        self.set_active_tickers(active_tickers_)\n",
    "        self.set_markets(list(set(markets)))\n",
    "    \n",
    "    def api_selected_tickers(self):\n",
    "        #Se recarga el tickers_config para info actualizada de los tickers.\n",
    "        self.initialize_config_tickers()\n",
    "        # Se despliegan los tickers activos en la UI para que el usuario elija qué tickers quiere actualizar el data.\n",
    "        ticker_list = self.get_tickers()\n",
    "        self.set_selected_tickers(ticker_list[0:3])\n",
    "        \n",
    "        #return self.get_active_tickers() #TODO\n",
    "    \n",
    "    def update_timeseries_download_date(self, selected_tickers_to_update):\n",
    "        config_ticker_list = self.get_tickers_config()\n",
    "        today = date.today()\n",
    "        # LAs fechas se guardan en formato %m-%d-%Y\n",
    "        [t.update({'data_update':today.strftime(\"%m-%d-%Y\")}) for t in config_ticker_list['data'] if t in selected_tickers_to_update]\n",
    "        self.set_tickers_config(config_ticker_list)\n",
    "        self.save_tickers_config()\n",
    "         \n",
    "    def load_ticker_data(self, file_name):\n",
    "        return self.__myHousekeeper.csv_to_df(self.__dir_list,\n",
    "                                              file_name)\n",
    "    \n",
    "    def save_ticker_data(self, file_name, data):\n",
    "        self.__myHousekeeper.df_to_csv(self.__dir_list,\n",
    "                                       self.__upper_stages, file_name, data)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "class DataManager_YahooFinance(DataManager):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    \n",
    "    def download_ticker_data_from_scratch(self, ticker, ticker_key):\n",
    "        print('Downloading from scratch historic data of: ' + ticker)\n",
    "        data_csv = yf.download(ticker)\n",
    "        data_csv.insert(loc=0, column='Date', value=pd.to_datetime(data_csv.index, errors='coerce'))\n",
    "        data_csv['Date'] = [time.date() for time in data_csv['Date']]\n",
    "        data_csv.reset_index(drop=True, inplace=True)\n",
    "        self.save_ticker_data(ticker_key,data_csv )\n",
    "        return data_csv\n",
    "    \n",
    "    def download_ticker_data_from_last_date(self, ticker, ticker_key, start_date):\n",
    "        print('Updating historic data of: ' + ticker)\n",
    "        # 1. Descargar datos desde la ultima fecha\n",
    "        data_csv = yf.download(ticker, start = start_date)\n",
    "        data_csv.insert(loc=0, column='Date', value=pd.to_datetime(data_csv.index, errors='coerce'))\n",
    "        data_csv['Date'] = [time.date() for time in data_csv['Date']]\n",
    "        print('Downloaded(sessions)', len(data_csv))\n",
    "        # 2. Cargar el csv\n",
    "        data_csv_local = DM_YF.load_ticker_data(ticker_key)\n",
    "        # 3. Apendear los datos que faltan, resetear el index y esta será la nueva varaible data_csv\n",
    "        data_csv = pd.concat([data_csv_local, data_csv], ignore_index = True)\n",
    "        data_csv.reset_index(drop=True, inplace=True)\n",
    "        data_csv.drop(data_csv.columns[0], axis = 1, inplace = True)\n",
    "        # 4. Guardar los datos sobreescribiendo el archivo anterior\n",
    "        self.save_ticker_data(ticker_key, data_csv)\n",
    "        #return data_csv\n",
    "    \n",
    "    def last_date_download(self, ticker_dict):\n",
    "        # Local variables\n",
    "        last_date_str_ = ticker_dict['data_update']\n",
    "        ticker_key_ = ticker_dict['tickerKey']\n",
    "        ticker = ticker_dict['feeds']['ticker']\n",
    "        # 3 casos: A) last_date is None -> from scratch, B) last >= today -> no hay descarga C) start < today (else) -> download_ticker_data_from_last_date\n",
    "        if last_date_str_ is None: # Aquí va un download_from_scratch\n",
    "            print(ticker + \" is not found in database, adding ----\")\n",
    "            #data_csv = yf.download(ticker) # Aquí va un download_from_scratch\n",
    "            self.download_ticker_data_from_scratch(ticker, ticker_key_)\n",
    "            return\n",
    "        now = datetime.now()\n",
    "        last_date = datetime.strptime(last_date_str_, '%m-%d-%Y')\n",
    "        delta = now - last_date\n",
    "        start_date = last_date + timedelta(days=+1)\n",
    "        if delta.days <= 0: # Aquí no hay download\n",
    "            print('Data of ', ticker_key_ ,'is already updated')\n",
    "            return\n",
    "        else: # Función download_ticker_data_from_last_date\n",
    "            self.download_ticker_data_from_last_date(ticker, ticker_key_, start_date)\n",
    "            delta = now - start_date\n",
    "            print('Downloaded(days): ', delta.days)\n",
    "            #return data_csv\n",
    "    \n",
    "    \n",
    "    def timeseries_download_manager(self, ticker_dict):\n",
    "        if self.get_last_date_flag(): # From last date\n",
    "            print('Download ', ticker_dict['tickerKey'],' from last updated_date')\n",
    "            self.last_date_download(ticker_dict)\n",
    "        else: # From scratch\n",
    "            print('Download', ticker_dict['tickerKey'],' from scratch')\n",
    "            self.download_ticker_data_from_scratch(ticker_dict['feeds']['ticker'],ticker_dict['tickerKey'])\n",
    "        \n",
    "    \n",
    "    def download_selected_tickers(self):\n",
    "        # Se almacenan los tickers que van a se actualizados y se guarda la fecha de actualización en el ticker_config. \n",
    "        # 1.- Almacenar selected_Tickers from user selection and a default option.\n",
    "        #selected_tickers_list = self.api_active_tickers()\n",
    "        self.api_selected_tickers()\n",
    "        #2.- Establecer el tipo de descarga: last_date(True) / from scratch(False, default) \n",
    "        self.set_last_date_flag(False)\n",
    "        #3.- Descargar los selected_tickers. Enganchar con timeseries_download_manager\n",
    "        [self.timeseries_download_manager(t) for t in self.get_selected_tickers()]\n",
    "        # 4.- Actualizar data_update en tickers_config de los tickers descargados\n",
    "        self.update_timeseries_download_date(self.get_selected_tickers())\n",
    "    \n",
    "    \n",
    "    def download_market_data(self, markets, _last_date_flag): #TODO: especificar el subconjunto en selected tickers. Para que se actualice la fecha data_update\n",
    "        print('Download market ticker')\n",
    "        #1.- Almacenar en selected_ticker los tickers correspondientes a un market\n",
    "        #Se recarga el tickers_config para info actualizada de los tickers.\n",
    "        self.initialize_config_tickers()\n",
    "        # Se despliegan los tickers activos en la UI para que el usuario elija qué tickers quiere actualizar el data.\n",
    "        active_ticker_list = self.get_active_tickers()\n",
    "        ticker_list = [t for t in active_ticker_list if t['market'] in markets]\n",
    "        self.set_selected_tickers(ticker_list)\n",
    "        #2.- Establecer el tipo de descarga: last_date(True) / from scratch(False, default) \n",
    "        self.set_last_date_flag(_last_date_flag)\n",
    "        #3.- Descargar los selected_tickers. Enganchar con timeseries_download_manager\n",
    "        #tickers = self.get_active_tickers()\n",
    "        #[DM_YF.download_ticker_data_from_scratch(t['feeds']['ticker'], t['tickerKey']) for t in tickers if t['market'] in markets]\n",
    "        [self.timeseries_download_manager(t) for t in self.get_selected_tickers()]\n",
    "        # 4.- Actualizar data_update en tickers_config de los tickers descargados\n",
    "        self.update_timeseries_download_date(self.get_selected_tickers())\n",
    "        \n",
    "    def download_all_markets(self):\n",
    "        print('Download ALL MARKETS')\n",
    "        self.download_market_data(self.get_markets())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/tickers_config.json\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/tickers_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('30/03/2021 20:01:20', ['SP500', 'IBEX35', 'DAX30', 'CAC40'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DM_YF = DataManager_YahooFinance()\n",
    "DM_YF.load_tickers_config() # Cargar archivo \"tickers_config.json\"\n",
    "data = DM_YF.get_tickers_config() # Obtener variable __tickers_config_list\n",
    "#DM_YF.initialize_metadata()\n",
    "DM_YF.initialize_config_tickers() # initialize_metadata(timestamp and set tickers_list) and set __active_tickers_list \n",
    "DM_YF.get_timestamp(), DM_YF.get_markets()\n",
    "#print(DM_YF.start_date_calculation(\"03-24-2021\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/tickers_config.json\n",
      "Download ANA.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: ANA.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/ANA.MC.TT.csv\n",
      "Download ACX.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: ACX.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/ACX.MC.TT.csv\n",
      "Download ACS.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: ACS.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/ACS.MC.TT.csv\n",
      "Root dir:  C:\\Users\\alvaro\\Repos\\Python_knowledge\\Appdatabase project\n",
      "parent_dir_path:  C:\\Users\\alvaro\\Repos\\Python_knowledge\\Appdatabase project\n",
      "dir_list:  ['Data', 'Tickers', 'Dummy1']\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "file path:  C:\\Users\\alvaro\\Repos\\Python_knowledge\\Appdatabase project/Data/Tickers/Dummy1/tickers_config.json\n"
     ]
    }
   ],
   "source": [
    "DM_YF.download_selected_tickers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download market ticker\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/tickers_config.json\n",
      "Download ANA.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: ANA.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/ANA.MC.TT.csv\n",
      "Download ACX.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: ACX.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/ACX.MC.TT.csv\n",
      "Download ACS.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: ACS.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/ACS.MC.TT.csv\n",
      "Download AENA.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: AENA.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/AENA.MC.TT.csv\n",
      "Download ALM.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: ALM.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/ALM.MC.TT.csv\n",
      "Download AMS.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: AMS.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/AMS.MC.TT.csv\n",
      "Download MTS.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: MTS.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/MTS.MC.TT.csv\n",
      "Download SAB.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: SAB.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/SAB.MC.TT.csv\n",
      "Download SAN.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: SAN.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/SAN.MC.TT.csv\n",
      "Download BKIA.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: BKIA.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/BKIA.MC.TT.csv\n",
      "Download BKT.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: BKT.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/BKT.MC.TT.csv\n",
      "Download BBVA.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: BBVA.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/BBVA.MC.TT.csv\n",
      "Download CABK.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: CABK.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/CABK.MC.TT.csv\n",
      "Download CLNX.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: CLNX.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/CLNX.MC.TT.csv\n",
      "Download CIE.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: CIE.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/CIE.MC.TT.csv\n",
      "Download ENG.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: ENG.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/ENG.MC.TT.csv\n",
      "Download ELE.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: ELE.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/ELE.MC.TT.csv\n",
      "Download FER.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: FER.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/FER.MC.TT.csv\n",
      "Download GRF.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: GRF.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/GRF.MC.TT.csv\n",
      "Download IAG.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: IAG.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/IAG.MC.TT.csv\n",
      "Download IBE.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: IBE.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/IBE.MC.TT.csv\n",
      "Download ITX.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: ITX.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/ITX.MC.TT.csv\n",
      "Download IDR.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: IDR.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/IDR.MC.TT.csv\n",
      "Download COL.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: COL.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/COL.MC.TT.csv\n",
      "Download MAP.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: MAP.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/MAP.MC.TT.csv\n",
      "Download MEL.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: MEL.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/MEL.MC.TT.csv\n",
      "Download MRL.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: MRL.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/MRL.MC.TT.csv\n",
      "Download NTGY.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: NTGY.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/NTGY.MC.TT.csv\n",
      "Download PHM.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: PHM.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/PHM.MC.TT.csv\n",
      "Download REE.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: REE.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/REE.MC.TT.csv\n",
      "Download REP.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: REP.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/REP.MC.TT.csv\n",
      "Download SGRE.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: SGRE.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/SGRE.MC.TT.csv\n",
      "Download SLR.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: SLR.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/SLR.MC.TT.csv\n",
      "Download TEF.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: TEF.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/TEF.MC.TT.csv\n",
      "Download VIS.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: VIS.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/VIS.MC.TT.csv\n",
      "Download ^IBEX  from scratch\n",
      "Downloading from scratch historic data of: ^IBEX\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/^IBEX.csv\n",
      "Root dir:  C:\\Users\\alvaro\\Repos\\Python_knowledge\\Appdatabase project\n",
      "parent_dir_path:  C:\\Users\\alvaro\\Repos\\Python_knowledge\\Appdatabase project\n",
      "dir_list:  ['Data', 'Tickers', 'Dummy1']\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "file path:  C:\\Users\\alvaro\\Repos\\Python_knowledge\\Appdatabase project/Data/Tickers/Dummy1/tickers_config.json\n"
     ]
    }
   ],
   "source": [
    "DM_YF.download_market_data('IBEX35', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = DM_YF.get_tickers()\n",
    "tickers[3]['data_update']\n",
    "#print(tickers[3]['data_update'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DM_YF.timeBounded_download('ACX.MC', 'ACX.MC.TT',\"03-23-2021\")\n",
    "DM_YF.timeBounded_download(tickers[4]['ticker'], tickers[4]['tickerKey'],tickers[4]['data_update'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DM_YF.download_ticker_data_from_last_date('ACX.MC', 'ACX.MC.TT',\"2021-03-24\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_date_definition(self):\n",
    "    return start_date\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date_str = a[0]['data_update']\n",
    "last_date = datetime.strptime(last_date_str, '%m-%d-%Y')\n",
    "print(last_date)\n",
    "start_date = last_date + timedelta(days=+1)\n",
    "print(start_date)\n",
    "print(start_date >= datetime.now())\n",
    "#start_date = None\n",
    "ticker = 'ANA.MC'\n",
    "def timeBounded_download(self, ticker, ticker_key, last_date):\n",
    "    # Se activa opción de descargar desde la última fecha \n",
    "    # 3 casos: A) start is None, B) start >= today C) start < today\n",
    "    delta = now-start_date\n",
    "    if start_date is None: # Aquí va un download_from_scratch\n",
    "        print(ticker + \" is not found in database, adding ----\")\n",
    "        #data_csv = yf.download(ticker) # Aquí va un download_from_scratch\n",
    "        self.download_ticker_data_from_scratch(ticker, ticker_key)\n",
    "    elif delta.days <= 0: # Aquí no hay download\n",
    "        print('Data is already updated')\n",
    "        return\n",
    "    else: # Aquí un download_from_last_date\n",
    "        #data_csv = yf.download(ticker, start = start_date)\n",
    "        # Función para calcular la start_date\n",
    "        start_date = self.start_date_calculation(last_date)\n",
    "        # Función download_ticker_data_from_last_date\n",
    "        self.download_ticker_data_from_last_date(ticker, ticker_key, start_date)\n",
    "        delta = now-start_date\n",
    "        print('Downloaded(days): ', delta.days)\n",
    "    return data_csv\n",
    "    \n",
    "    #return data_csv = yf.download(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = timeBounded_download('ACS')\n",
    "data_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date_str = a[0]['data_update']\n",
    "print(type(last_date))\n",
    "#last_date = pd.to_datetime(last_date, errors='coerce')\n",
    "last_date = datetime.strptime(last_date_str, '%m-%d-%Y')\n",
    "#print(last_date)\n",
    "start_date = last_date + timedelta(days=-2)\n",
    "#start_date = pd.to_datetime(start_date, errors='coerce')\n",
    "#start_date = None\n",
    "print(last_date)\n",
    "print(start_date), print(type(start_date))\n",
    "today = date.today()\n",
    "now = datetime.now()\n",
    "#print(today), print(type(today))\n",
    "print(now), print(type(now))\n",
    "#print(today.strftime(\"%d-%m-%Y\"))\n",
    "print(start_date >= now)\n",
    "delta = start_date - now\n",
    "print(delta.days)\n",
    "#data_csv = yf.download('ANA.MC', start = start_date)\n",
    "#data_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = DM_YF.get_active_tickers()\n",
    "a[0]['data_update'], a[2]['data_update']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data_csv = yf.download('ANA.MC')\n",
    "last_date = a[0]['data_update']\n",
    "last_date = pd.to_datetime(last_date, errors='coerce')\n",
    "last_date.date()\n",
    "start_date = last_date + timedelta(days=1)\n",
    "start_date_1 = last_date + timedelta(days=-2)\n",
    "last_date, start_date, start_date_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = yf.download('ANA.MC', start=start_date_1, end=datetime.now())\n",
    "#data_csv = yf.download('ANA.MC')\n",
    "data_csv.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date = a[0]['data_update'] \n",
    "last_date = pd.to_datetime(last_date, errors='coerce')\n",
    "last_date = last_date + timedelta(days=-2)\n",
    "last_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start = last_date\n",
    "start = None\n",
    "end = end=datetime.now()\n",
    "def fun1(start = start, end = end):\n",
    "    if start is not None:\n",
    "        data_csv = yf.download('ANA.MC',start,  end)\n",
    "    else:\n",
    "        data_csv = yf.download('ANA.MC')\n",
    "    return data_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = fun1()\n",
    "type(data_csv)\n",
    "data_csv.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_dict = {'start':last_date}\n",
    "def fun2(**kwargs):\n",
    "    data_csv = yf.download('ANA.MC', **kwargs)\n",
    "        #data_csv = yf.download('ANA.MC',start,  end)\n",
    "    return data_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = fun2()\n",
    "type(data_csv)\n",
    "data_csv.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_dict = {'start':last_date}\n",
    "data_csv = yf.download('ANA.MC', **date_dict)\n",
    "data_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = yf.download('ANA.MC', end=datetime.now())\n",
    "#data_csv = yf.download('ANA.MC')\n",
    "data_csv.head(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitectura descarga tickers\n",
    "fdsa\n",
    "fdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(a = 1):\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1(424)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers[0]['feeds']['ticker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ticker = DM_YF.download_ticker_data(tickers[0]['feeds']['ticker'], tickers[0]['tickerKey'])\n",
    "#data_ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ticker.head(), type(data_ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DM_YF.download_market_data('IBEX35')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markets = DM_YF.get_markets()\n",
    "markets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DM_YF.download_all_markets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Last_Data(ticker, market, backup):\n",
    "    # Check if the market is in the database. In case that not exist, WARNING and return empty csv.\n",
    "    markets = os.listdir('./Data/')\n",
    "    if market not in markets:\n",
    "        print(\"There is no market in database, please add new market and fill with data\")\n",
    "        data_csv = []\n",
    "        return data_csv\n",
    "\n",
    "    # If market exists between the markets loaded, then go to market folder and load shares list\n",
    "    # Get shares of a market from the folder that it is stored\n",
    "    shares = os.listdir('./Data/' + market)\n",
    "    shares = [s.replace('.csv', '') for s in shares]\n",
    "\n",
    "    # Function to download full historic data, used for backup and in firts time download\n",
    "    def dowload_historic_data(ticker):\n",
    "        print('Downloading historic data of: ' + ticker)\n",
    "        data_csv = yf.download(ticker)\n",
    "        data_csv.insert(loc=0, column='Date', value=pd.to_datetime(data_csv.index, errors='coerce'))\n",
    "        data_csv['Date'] = [time.date() for time in data_csv['Date']]\n",
    "        data_csv.reset_index(drop=True, inplace=True)\n",
    "        return data_csv\n",
    "\n",
    "    if ticker in shares:  # IN CASE THAT THE SHARE EXISTS PREVIOUSLY. If share in LISTA_ACCIONES\n",
    "        if backup:  # BACK UP CASE. Download data from the historic in YahooFinance\n",
    "            data_csv = dowload_historic_data(ticker)\n",
    "        else:  # NO BACK UP CASE. Download data from the last date updated\n",
    "            # Load de data\n",
    "            print(\"-------------------------------------------\")\n",
    "            print(ticker + \" exists in Database. \" + \"Opening \" + ticker)\n",
    "            data_csv = pd.read_csv(\"./Data/\" + market + \"/\" + ticker + \".csv\")\n",
    "            # Read and get the date of the last day\n",
    "            last_date = data_csv[\"Date\"].iloc[-1]\n",
    "            last_date = pd.to_datetime(last_date, errors='coerce')\n",
    "            last_date.date()\n",
    "            # If last date is today, then return the file with no modifications\n",
    "            if last_date >= date.today():\n",
    "                print(\"File is already updated. No modifications.\")\n",
    "                # print (\"-------------------------------------------\")\n",
    "                return data_csv\n",
    "            # print(\"Updating \" + ticker +' from '+ last_date +' until '+date.today()+'(today)' )\n",
    "            print('Updating ' + ticker + ' until today')\n",
    "            start_date = last_date + timedelta(days=1)\n",
    "            # Download data from the selected date\n",
    "            data = yf.download(ticker, start=start_date, end=datetime.now())\n",
    "            data.insert(loc=0, column='Date', value=pd.to_datetime(data.index, errors='coerce'))\n",
    "            data['Date'] = [time.date() for time in data['Date']]\n",
    "            data.reset_index(drop=True, inplace=True)\n",
    "            data_csv = data_csv.append(data, ignore_index=True)\n",
    "    else:  # IN CASE THAT THE SHARE DOESNT EXISTS PREVIOUSLY. Download historic data.\n",
    "        print(ticker + \" is not found in database, adding ----\")\n",
    "        data_csv = dowload_historic_data(ticker)\n",
    "    return data_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_ticker_data_from_last_date(ticker, ticker_key, start_date):\n",
    "        print('Updating historic data of: ' + ticker)\n",
    "        # 1. Descargar datos desde la ultima fecha\n",
    "        data_csv = yf.download(ticker, start = start_date)\n",
    "        data_csv.insert(loc=0, column='Date', value=pd.to_datetime(data_csv.index, errors='coerce'))\n",
    "        data_csv['Date'] = [time.date() for time in data_csv['Date']]\n",
    "        # 2. Cargar el csv\n",
    "        #data_csv_local = DM_YF.load_ticker_data(ticker_key)\n",
    "        data_csv_local = pd.read_csv('./Data/Tickers/Dummy1/ACX.MC.TT.csv')\n",
    "        # 3. Apendear los datos que faltan, resetear el index y esta será la nueva varaible data_csv\n",
    "        print(type(data_csv_local)),print(type(data_csv))\n",
    "        data_csv = pd.concat([data_csv_local, data_csv], ignore_index = True)\n",
    "        data_csv.reset_index(drop=True, inplace=True)\n",
    "        data_csv.drop(data_csv.columns[0], axis = 1, inplace = True)\n",
    "        #data_csv.drop(1)\n",
    "        # 4. Guardar los datos\n",
    "        #self.save_ticker_data(ticker_key,data_csv )\n",
    "        return data_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_ticker_data_from_last_date('ANA.MC', 'ANA.MC.TT',\"2021-03-23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = DM_YF.get_tickers()\n",
    "a[0]['data_update'], a[9]['data_update']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date = a[0]['data_update']\n",
    "print(type(last_date))\n",
    "#last_date = pd.to_datetime(last_date, errors='coerce')\n",
    "last_date = datetime.strptime(last_date, '%m-%d-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DM_YF.load_ticker_data('ACX.MC.TT')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('./Data/Tickers/Dummy1/ACX.MC.TT.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_market_data(self, market):\n",
    "    print('Download market ticker')\n",
    "    tickers = self.get_active_tickers()\n",
    "    [DM_YF.download_ticker_data(t['feeds']['ticker'], t['tickerKey']) for t in tickers if t['market'] in markets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = DM_YF.get_active_tickers() # Get active tickers\n",
    "#tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markets = ['IBEX35']\n",
    "[print(t) for t in tickers if t['market'] in markets]\n",
    "\n",
    "DM_YF.download_ticker_data(tickers[0]['feeds']['ticker'], tickers[0]['tickerKey'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[DM_YF.download_ticker_data(t['feeds']['ticker'], t['tickerKey']) for t in tickers if t['market'] in markets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import housekeeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = ['Data']\n",
    "file_name = 'tickers_config.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myHousekeeper = housekeeper.instance_class() \n",
    "data = myHousekeeper.load_json_to_list(dir_list, file_name)\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markets = []\n",
    "active_tickers = []\n",
    "for d in data:\n",
    "    markets.append(d['market'])\n",
    "    if d['active_type']=='stock' and d['active_flag']:\n",
    "        active_tickers.append(d)\n",
    "    elif d['active_type']=='ETF':\n",
    "        active_tickers.append(d)\n",
    "markets = set(markets)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markets, active_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0:2]['market']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyValList = ['a']\n",
    "data[0]['market'] in keyValList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data[0]['active_flag']:\n",
    "    print('hola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    ">>> exampleSet = [{'type':'type1'},{'type':'type2'},{'type':'type2'}, {'type':'type3'}]\n",
    ">>> keyValList = ['type2','type3']\n",
    ">>> expectedResult = [d for d in exampleSet if d['type'] in keyValList]\n",
    ">>> expectedResult\n",
    "[{'type': 'type2'}, {'type': 'type2'}, {'type': 'type3'}]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    ">>> list(filter(lambda d: d['type'] in keyValList, exampleSet))\n",
    "[{'type': 'type2'}, {'type': 'type2'}, {'type': 'type3'}]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.finxter.com/how-to-filter-a-list-of-dictionaries-in-python/#Where_to_Go_From_Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DM_YF.get_tickers_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DM_YF.load_tickers_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = ['a', 'b', 'c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list.pop(0)\n",
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list.pop(0)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
