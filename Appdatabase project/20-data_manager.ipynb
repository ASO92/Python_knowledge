{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modulo appdata: Datamanagers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from datetime import datetime, timedelta, date\n",
    "import os\n",
    "\n",
    "import housekeeper\n",
    "\n",
    "class DataManager(ABC):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.__myHousekeeper = housekeeper.instance_class()\n",
    "        self.__config_filename = \"tickers_config.json\"\n",
    "        self.__dir_list = ['Data', 'Tickers', 'Dummy1']\n",
    "        self.__upper_stages = 0\n",
    "        self.__tickers_config_list = []\n",
    "        self.__tickers_list = []\n",
    "        self.__active_tickers_list = []\n",
    "        self.__selected_tickers_list = []\n",
    "        self.__timestamp = ''\n",
    "        self.__markets = []\n",
    "        self.__last_date_flag = False\n",
    "    \n",
    "\n",
    "    def get_config_filename(self):\n",
    "        return self.__config_filename\n",
    "    \n",
    "    def set_config_filename(self, config_filename):\n",
    "        self.__config_filename = config_filename\n",
    "        \n",
    "    def get_dir_list(self):\n",
    "        return self.__dir_list\n",
    "    \n",
    "    def set_dir_list(self, dir_list):\n",
    "        self.__dir_list = dir_list\n",
    "    \n",
    "    def get_upper_stages(self):\n",
    "        return self.__upper_stages\n",
    "    \n",
    "    def set_upper_stages(self, upper_stages):\n",
    "        self.__upper_stages = dir_list\n",
    "        \n",
    "    def get_last_date_flag(self):\n",
    "        return self.__last_date_flag\n",
    "    \n",
    "    def set_last_date_flag(self, last_date_flag):\n",
    "        self.__last_date_flag = last_date_flag\n",
    "        \n",
    "    def get_tickers_config(self):\n",
    "        return self.__tickers_config_list\n",
    "    \n",
    "    def set_tickers_config(self, tickers_config_list):\n",
    "        self.__tickers_config_list = tickers_config_list\n",
    "    \n",
    "    def get_tickers(self):\n",
    "        return self.__tickers_list\n",
    "    \n",
    "    def set_tickers(self, tickers_list):\n",
    "        self.__tickers_list = tickers_list\n",
    "        \n",
    "    def get_active_tickers(self):\n",
    "        return self.__active_tickers_list\n",
    "    \n",
    "    def set_active_tickers(self, active_tickers_list):\n",
    "        self.__active_tickers_list = active_tickers_list\n",
    "        \n",
    "    def get_selected_tickers(self):\n",
    "        return self.__selected_tickers_list\n",
    "    \n",
    "    def set_selected_tickers(self, selected_tickers_list):\n",
    "        self.__selected_tickers_list = selected_tickers_list\n",
    "    \n",
    "    def get_timestamp(self):\n",
    "        return self.__timestamp\n",
    "    \n",
    "    def set_timestamp(self, timestamp):\n",
    "        self.__timestamp = timestamp\n",
    "    \n",
    "    def get_markets(self):\n",
    "        return self.__markets\n",
    "    \n",
    "    def set_markets(self, markets):\n",
    "        self.__markets = markets\n",
    "    \n",
    "    def load_tickers_config(self):\n",
    "        data = self.__myHousekeeper.load_json_to_list(self.__dir_list, self.__config_filename)\n",
    "        self.set_tickers_config(data)\n",
    "        \n",
    "    def save_tickers_config(self):\n",
    "        #No invocar a esta función sin previamente haber cargado tickers_config. O se sobreescribe tickers_config\n",
    "        tickers_config = self.get_tickers_config()\n",
    "        self.__myHousekeeper.list_dict_to_json(self.get_dir_list(), \n",
    "                                               self.get_upper_stages(), \n",
    "                                               self.get_config_filename(), \n",
    "                                               self.get_tickers_config())\n",
    "    \n",
    "    def initialize_metadata(self):\n",
    "        self.load_tickers_config()\n",
    "        data = self.get_tickers_config()\n",
    "        self.set_timestamp(data['metadata'][0]['timestamp'])\n",
    "        self.set_tickers(data['data'])\n",
    "        \n",
    "    def initialize_config_tickers(self):\n",
    "        # Get markets, get active_tickers\n",
    "        markets = []\n",
    "        active_tickers_ = []\n",
    "        self.initialize_metadata()\n",
    "        data = self.get_tickers()\n",
    "        for d in data:\n",
    "            markets.append(d['market'])\n",
    "            if d['active_type']=='stock' and d['active_flag']:\n",
    "                active_tickers_.append(d)\n",
    "            elif d['active_type']=='ETF':\n",
    "                active_tickers_.append(d)\n",
    "        self.set_active_tickers(active_tickers_)\n",
    "        self.set_markets(list(set(markets)))\n",
    "    \n",
    "    def api_selected_tickers(self):\n",
    "        #Se recarga el tickers_config para info actualizada de los tickers.\n",
    "        self.initialize_config_tickers()\n",
    "        # Se despliegan los tickers activos en la UI para que el usuario elija qué tickers quiere actualizar el data.\n",
    "        ticker_list = self.get_tickers()\n",
    "        self.set_selected_tickers(ticker_list[0:3])\n",
    "        \n",
    "        #return self.get_active_tickers() #TODO\n",
    "    \n",
    "    def update_timeseries_download_date(self, selected_tickers_to_update):\n",
    "        config_ticker_list = self.get_tickers_config()\n",
    "        today = date.today()\n",
    "        # LAs fechas se guardan en formato %m-%d-%Y\n",
    "        [t.update({'data_update':today.strftime(\"%m-%d-%Y\")}) for t in config_ticker_list['data'] if t in selected_tickers_to_update]\n",
    "        self.set_tickers_config(config_ticker_list)\n",
    "        self.save_tickers_config()\n",
    "         \n",
    "    def load_ticker_data(self, file_name):\n",
    "        return self.__myHousekeeper.csv_to_df(self.__dir_list,\n",
    "                                              file_name)\n",
    "    \n",
    "    def save_ticker_data(self, file_name, data):\n",
    "        self.__myHousekeeper.df_to_csv(self.__dir_list,\n",
    "                                       self.__upper_stages, file_name, data)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "class DataManager_YahooFinance(DataManager):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    \n",
    "    def download_ticker_data_from_scratch(self, ticker, ticker_key):\n",
    "        print('Downloading from scratch historic data of: ' + ticker)\n",
    "        data_csv = yf.download(ticker)\n",
    "        data_csv.insert(loc=0, column='Date', value=pd.to_datetime(data_csv.index, errors='coerce'))\n",
    "        data_csv['Date'] = [time.date() for time in data_csv['Date']]\n",
    "        data_csv.reset_index(drop=True, inplace=True)\n",
    "        self.save_ticker_data(ticker_key,data_csv )\n",
    "        return data_csv\n",
    "    \n",
    "    def download_ticker_data_from_last_date(self, ticker, ticker_key, start_date):\n",
    "        print('Updating historic data of: ' + ticker)\n",
    "        # 1. Descargar datos desde la ultima fecha\n",
    "        data_csv = yf.download(ticker, start = start_date)\n",
    "        data_csv.insert(loc=0, column='Date', value=pd.to_datetime(data_csv.index, errors='coerce'))\n",
    "        data_csv['Date'] = [time.date() for time in data_csv['Date']]\n",
    "        print('Downloaded(sessions)', len(data_csv))\n",
    "        # 2. Cargar el csv\n",
    "        data_csv_local = DM_YF.load_ticker_data(ticker_key)\n",
    "        # 3. Apendear los datos que faltan, resetear el index y esta será la nueva varaible data_csv\n",
    "        data_csv = pd.concat([data_csv_local, data_csv], ignore_index = True)\n",
    "        data_csv.reset_index(drop=True, inplace=True)\n",
    "        data_csv.drop(data_csv.columns[0], axis = 1, inplace = True)\n",
    "        # 4. Guardar los datos sobreescribiendo el archivo anterior\n",
    "        self.save_ticker_data(ticker_key, data_csv)\n",
    "        #return data_csv\n",
    "    \n",
    "    def last_date_download(self, ticker_dict):\n",
    "        # Local variables\n",
    "        last_date_str_ = ticker_dict['data_update']\n",
    "        ticker_key_ = ticker_dict['tickerKey']\n",
    "        ticker = ticker_dict['feeds']['ticker']\n",
    "        # 3 casos: A) last_date is None -> from scratch, B) last >= today -> no hay descarga C) start < today (else) -> download_ticker_data_from_last_date\n",
    "        if last_date_str_ is None: # Aquí va un download_from_scratch\n",
    "            print(ticker + \" is not found in database, adding ----\")\n",
    "            #data_csv = yf.download(ticker) # Aquí va un download_from_scratch\n",
    "            self.download_ticker_data_from_scratch(ticker, ticker_key_)\n",
    "            return\n",
    "        now = datetime.now()\n",
    "        last_date = datetime.strptime(last_date_str_, '%m-%d-%Y')\n",
    "        delta = now - last_date\n",
    "        start_date = last_date + timedelta(days=+1)\n",
    "        if delta.days <= 0: # Aquí no hay download\n",
    "            print('Data of ', ticker_key_ ,'is already updated')\n",
    "            return\n",
    "        else: # Función download_ticker_data_from_last_date\n",
    "            self.download_ticker_data_from_last_date(ticker, ticker_key_, start_date)\n",
    "            delta = now - start_date\n",
    "            print('Downloaded(days): ', delta.days)\n",
    "            #return data_csv\n",
    "    \n",
    "    \n",
    "    def timeseries_download_manager(self, ticker_dict):\n",
    "        if self.get_last_date_flag(): # From last date\n",
    "            print('Download ', ticker_dict['tickerKey'],' from last updated_date')\n",
    "            self.last_date_download(ticker_dict)\n",
    "        else: # From scratch\n",
    "            print('Download', ticker_dict['tickerKey'],' from scratch')\n",
    "            self.download_ticker_data_from_scratch(ticker_dict['feeds']['ticker'],ticker_dict['tickerKey'])\n",
    "        \n",
    "    \n",
    "    def download_selected_tickers(self):\n",
    "        # Se almacenan los tickers que van a se actualizados y se guarda la fecha de actualización en el ticker_config. \n",
    "        # 1.- Almacenar selected_Tickers from user selection and a default option.\n",
    "        #selected_tickers_list = self.api_active_tickers()\n",
    "        self.api_selected_tickers()\n",
    "        #2.- Establecer el tipo de descarga: last_date(True) / from scratch(False, default) \n",
    "        self.set_last_date_flag(False)\n",
    "        #3.- Descargar los selected_tickers. Enganchar con timeseries_download_manager\n",
    "        [self.timeseries_download_manager(t) for t in self.get_selected_tickers()]\n",
    "        # 4.- Actualizar data_update en tickers_config de los tickers descargados\n",
    "        self.update_timeseries_download_date(self.get_selected_tickers())\n",
    "    \n",
    "    \n",
    "    def download_market_data(self, markets, _last_date_flag = False): #TODO: especificar el subconjunto en selected tickers. Para que se actualice la fecha data_update\n",
    "        print('Download market ticker')\n",
    "        #1.- Almacenar en selected_ticker los tickers correspondientes a un market\n",
    "        #Se recarga el tickers_config para info actualizada de los tickers.\n",
    "        self.initialize_config_tickers()\n",
    "        # Se despliegan los tickers activos en la UI para que el usuario elija qué tickers quiere actualizar el data.\n",
    "        active_ticker_list = self.get_active_tickers()\n",
    "        ticker_list = [t for t in active_ticker_list if t['market'] in markets]\n",
    "        self.set_selected_tickers(ticker_list)\n",
    "        #2.- Establecer el tipo de descarga: last_date(True) / from scratch(False, default) \n",
    "        self.set_last_date_flag(_last_date_flag)\n",
    "        #3.- Descargar los selected_tickers. Enganchar con timeseries_download_manager\n",
    "        #tickers = self.get_active_tickers()\n",
    "        #[DM_YF.download_ticker_data_from_scratch(t['feeds']['ticker'], t['tickerKey']) for t in tickers if t['market'] in markets]\n",
    "        [self.timeseries_download_manager(t) for t in self.get_selected_tickers()]\n",
    "        # 4.- Actualizar data_update en tickers_config de los tickers descargados\n",
    "        self.update_timeseries_download_date(self.get_selected_tickers())\n",
    "        \n",
    "    def download_all_markets(self):\n",
    "        print('Download ALL MARKETS')\n",
    "        self.initialize_config_tickers() # Inicializar config tickers, para llenar la variable markers, y porder obtener get_markets\n",
    "        self.download_market_data(self.get_markets())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/tickers_config.json\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Data/Tickers/Dummy1/tickers_config.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e8ead79e53b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mDM_YF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataManager_YahooFinance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mDM_YF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_tickers_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Cargar archivo \"tickers_config.json\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDM_YF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tickers_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Obtener variable __tickers_config_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#DM_YF.initialize_metadata()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mDM_YF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_config_tickers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# initialize_metadata(timestamp and set tickers_list) and set __active_tickers_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-02d3388b9b90>\u001b[0m in \u001b[0;36mload_tickers_config\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload_tickers_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__myHousekeeper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_json_to_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dir_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__config_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_tickers_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Repos\\Python_knowledge\\Appdatabase project\\housekeeper.py\u001b[0m in \u001b[0;36mload_json_to_list\u001b[1;34m(dir_list, file_name)\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[0mrelative_file_path_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'.'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_housekeeper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_file_path_in_nested_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrelative_file_path_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrelative_file_path_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Data/Tickers/Dummy1/tickers_config.json'"
     ]
    }
   ],
   "source": [
    "DM_YF = DataManager_YahooFinance()\n",
    "DM_YF.load_tickers_config() # Cargar archivo \"tickers_config.json\"\n",
    "data = DM_YF.get_tickers_config() # Obtener variable __tickers_config_list\n",
    "#DM_YF.initialize_metadata()\n",
    "DM_YF.initialize_config_tickers() # initialize_metadata(timestamp and set tickers_list) and set __active_tickers_list \n",
    "DM_YF.get_timestamp(), DM_YF.get_markets()\n",
    "#print(DM_YF.start_date_calculation(\"03-24-2021\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ANA.MC.TT',\n",
       " 'ACX.MC.TT',\n",
       " 'ACS.MC.TT',\n",
       " 'AENA.MC.TT',\n",
       " 'ALM.MC.TT',\n",
       " 'AMS.MC.TT',\n",
       " 'MTS.MC.TT',\n",
       " 'SAB.MC.TT',\n",
       " 'SAN.MC.TT',\n",
       " 'BKT.MC.TT',\n",
       " 'BBVA.MC.TT',\n",
       " 'CABK.MC.TT',\n",
       " 'CLNX.MC.TT',\n",
       " 'CIE.MC.TT',\n",
       " 'ENG.MC.TT',\n",
       " 'ELE.MC.TT',\n",
       " 'FER.MC.TT',\n",
       " 'FDR.MC.TT',\n",
       " 'GRF.MC.TT',\n",
       " 'IAG.MC.TT',\n",
       " 'IBE.MC.TT',\n",
       " 'ITX.MC.TT',\n",
       " 'IDR.MC.TT',\n",
       " 'COL.MC.TT',\n",
       " 'MAP.MC.TT',\n",
       " 'MEL.MC.TT',\n",
       " 'MRL.MC.TT',\n",
       " 'NTGY.MC.TT',\n",
       " 'PHM.MC.TT',\n",
       " 'REE.MC.TT',\n",
       " 'REP.MC.TT',\n",
       " 'SGRE.MC.TT',\n",
       " 'SLR.MC.TT',\n",
       " 'TEF.MC.TT',\n",
       " 'VIS.MC.TT',\n",
       " '^IBEX',\n",
       " 'ADS.DE.TT',\n",
       " 'ALV.DE.TT',\n",
       " 'BAS.DE.TT',\n",
       " 'BAYN.DE.TT',\n",
       " 'BMW.DE.TT',\n",
       " 'CON.DE.TT',\n",
       " '1COV.DE.TT',\n",
       " 'DAI.DE.TT',\n",
       " 'DHER.DE.TT',\n",
       " 'DBK.DE.TT',\n",
       " 'DB1.DE.TT',\n",
       " 'DPW.DE.TT',\n",
       " 'DTE.DE.TT',\n",
       " 'DWNI.DE.TT',\n",
       " 'EOAN.DE.TT',\n",
       " 'FRE.DE.TT',\n",
       " 'FME.DE.TT',\n",
       " 'HEI.DE.TT',\n",
       " 'HEN3.DE.TT',\n",
       " 'IFX.DE.TT',\n",
       " 'LIN.DE.TT',\n",
       " 'MRK.DE.TT',\n",
       " 'MTX.DE.TT',\n",
       " 'MUV2.DE.TT',\n",
       " 'RWE.DE.TT',\n",
       " 'SAP.DE.TT',\n",
       " 'SIE.DE.TT',\n",
       " 'ENR.DE.TT',\n",
       " 'VOW3.DE.TT',\n",
       " 'VNA.DE.TT',\n",
       " '^GDAXI',\n",
       " 'AC.PA.TT',\n",
       " 'AI.PA.TT',\n",
       " 'AIG.PA.TT',\n",
       " 'ALO.PA.TT',\n",
       " 'MT.PA.TT',\n",
       " 'CS.PA.TT',\n",
       " 'BNP.PA.TT',\n",
       " 'EN.PA.TT',\n",
       " 'CAP.PA.TT',\n",
       " 'CA.PA.TT',\n",
       " 'ACA.PA.TT',\n",
       " 'BN.PA.TT',\n",
       " 'EDF.PA.TT',\n",
       " 'EL.PA.TT',\n",
       " 'KER.PA.TT',\n",
       " 'LI.PA.TT',\n",
       " 'LHN.PA.TT',\n",
       " 'LR.PA.TT',\n",
       " 'MC.PA.TT',\n",
       " 'OR.PA.TT',\n",
       " 'ML.PA.TT',\n",
       " 'NO.PA.TT',\n",
       " 'OR.PA.TT',\n",
       " 'RI.PA.TT',\n",
       " 'PEU.PA.TT',\n",
       " 'PUB.PA.TT',\n",
       " 'RNO.PA.TT',\n",
       " 'SAF.PA.TT',\n",
       " 'SGO.PA.TT',\n",
       " 'SAN.PA.TT',\n",
       " 'SU.PA.TT',\n",
       " 'GLE.PA.TT',\n",
       " 'SOLB.PA.TT',\n",
       " 'TEC.PA.TT',\n",
       " 'FP.PA.TT',\n",
       " 'UL.PA.TT',\n",
       " 'VL.PA.TT',\n",
       " 'VIE.PA.TT',\n",
       " 'DG.PA.TT',\n",
       " 'VIV.PA.TT',\n",
       " '^FCHI',\n",
       " 'MMM',\n",
       " 'ABT',\n",
       " 'ABBV',\n",
       " 'ABMD',\n",
       " 'ACN',\n",
       " 'ATVI',\n",
       " 'ADBE',\n",
       " 'AMD',\n",
       " 'AAP',\n",
       " 'AES',\n",
       " 'AFL',\n",
       " 'A',\n",
       " 'APD',\n",
       " 'AKAM',\n",
       " 'ALK',\n",
       " 'ALB',\n",
       " 'ARE',\n",
       " 'ALXN',\n",
       " 'ALGN',\n",
       " 'ALLE',\n",
       " 'LNT',\n",
       " 'ALL',\n",
       " 'GOOGL',\n",
       " 'GOOG',\n",
       " 'MO',\n",
       " 'AMZN',\n",
       " 'AMCR',\n",
       " 'AEE',\n",
       " 'AAL',\n",
       " 'AEP',\n",
       " 'AXP',\n",
       " 'AIG',\n",
       " 'AMT',\n",
       " 'AWK',\n",
       " 'AMP',\n",
       " 'ABC',\n",
       " 'AME',\n",
       " 'AMGN',\n",
       " 'APH',\n",
       " 'ADI',\n",
       " 'ANSS',\n",
       " 'ANTM',\n",
       " 'AON',\n",
       " 'AOS',\n",
       " 'APA',\n",
       " 'AAPL',\n",
       " 'AMAT',\n",
       " 'APTV',\n",
       " 'ADM',\n",
       " 'ANET',\n",
       " 'AJG',\n",
       " 'AIZ',\n",
       " 'T',\n",
       " 'ATO',\n",
       " 'ADSK',\n",
       " 'ADP',\n",
       " 'AZO',\n",
       " 'AVB',\n",
       " 'AVY',\n",
       " 'BKR',\n",
       " 'BLL',\n",
       " 'BAC',\n",
       " 'BK',\n",
       " 'BAX',\n",
       " 'BDX',\n",
       " 'BRK.B',\n",
       " 'BBY',\n",
       " 'BIO',\n",
       " 'BIIB',\n",
       " 'BLK',\n",
       " 'BA',\n",
       " 'BKNG',\n",
       " 'BWA',\n",
       " 'BXP',\n",
       " 'BSX',\n",
       " 'BMY',\n",
       " 'AVGO',\n",
       " 'BR',\n",
       " 'BF.B',\n",
       " 'CHRW',\n",
       " 'COG',\n",
       " 'CDNS',\n",
       " 'CZR',\n",
       " 'CPB',\n",
       " 'COF',\n",
       " 'CAH',\n",
       " 'KMX',\n",
       " 'CCL',\n",
       " 'CARR',\n",
       " 'CTLT',\n",
       " 'CAT',\n",
       " 'CBOE',\n",
       " 'CBRE',\n",
       " 'CDW',\n",
       " 'CE',\n",
       " 'CNC',\n",
       " 'CNP',\n",
       " 'CERN',\n",
       " 'CF',\n",
       " 'SCHW',\n",
       " 'CHTR',\n",
       " 'CVX',\n",
       " 'CMG',\n",
       " 'CB',\n",
       " 'CHD',\n",
       " 'CI',\n",
       " 'CINF',\n",
       " 'CTAS',\n",
       " 'CSCO',\n",
       " 'C',\n",
       " 'CFG',\n",
       " 'CTXS',\n",
       " 'CLX',\n",
       " 'CME',\n",
       " 'CMS',\n",
       " 'KO',\n",
       " 'CTSH',\n",
       " 'CL',\n",
       " 'CMCSA',\n",
       " 'CMA',\n",
       " 'CAG',\n",
       " 'COP',\n",
       " 'ED',\n",
       " 'STZ',\n",
       " 'COO',\n",
       " 'CPRT',\n",
       " 'GLW',\n",
       " 'CTVA',\n",
       " 'COST',\n",
       " 'CCI',\n",
       " 'CSX',\n",
       " 'CMI',\n",
       " 'CVS',\n",
       " 'DHI',\n",
       " 'DHR',\n",
       " 'DRI',\n",
       " 'DVA',\n",
       " 'DE',\n",
       " 'DAL',\n",
       " 'XRAY',\n",
       " 'DVN',\n",
       " 'DXCM',\n",
       " 'FANG',\n",
       " 'DLR',\n",
       " 'DFS',\n",
       " 'DISCA',\n",
       " 'DISCK',\n",
       " 'DISH',\n",
       " 'DG',\n",
       " 'DLTR',\n",
       " 'D',\n",
       " 'DPZ',\n",
       " 'DOV',\n",
       " 'DOW',\n",
       " 'DTE',\n",
       " 'DUK',\n",
       " 'DRE',\n",
       " 'DD',\n",
       " 'DXC',\n",
       " 'EMN',\n",
       " 'ETN',\n",
       " 'EBAY',\n",
       " 'ECL',\n",
       " 'EIX',\n",
       " 'EW',\n",
       " 'EA',\n",
       " 'EMR',\n",
       " 'ENPH',\n",
       " 'ETR',\n",
       " 'EOG',\n",
       " 'EFX',\n",
       " 'EQIX',\n",
       " 'EQR',\n",
       " 'ESS',\n",
       " 'EL',\n",
       " 'ETSY',\n",
       " 'EVRG',\n",
       " 'ES',\n",
       " 'RE',\n",
       " 'EXC',\n",
       " 'EXPE',\n",
       " 'EXPD',\n",
       " 'EXR',\n",
       " 'XOM',\n",
       " 'FFIV',\n",
       " 'FB',\n",
       " 'FAST',\n",
       " 'FRT',\n",
       " 'FDX',\n",
       " 'FIS',\n",
       " 'FITB',\n",
       " 'FE',\n",
       " 'FRC',\n",
       " 'FISV',\n",
       " 'FLT',\n",
       " 'FLIR',\n",
       " 'FMC',\n",
       " 'F',\n",
       " 'FTNT',\n",
       " 'FTV',\n",
       " 'FBHS',\n",
       " 'FOXA',\n",
       " 'FOX',\n",
       " 'BEN',\n",
       " 'FCX',\n",
       " 'GPS',\n",
       " 'GRMN',\n",
       " 'IT',\n",
       " 'GNRC',\n",
       " 'GD',\n",
       " 'GE',\n",
       " 'GIS',\n",
       " 'GM',\n",
       " 'GPC',\n",
       " 'GILD',\n",
       " 'GL',\n",
       " 'GPN',\n",
       " 'GS',\n",
       " 'GWW',\n",
       " 'HAL',\n",
       " 'HBI',\n",
       " 'HIG',\n",
       " 'HAS',\n",
       " 'HCA',\n",
       " 'PEAK',\n",
       " 'HSIC',\n",
       " 'HSY',\n",
       " 'HES',\n",
       " 'HPE',\n",
       " 'HLT',\n",
       " 'HFC',\n",
       " 'HOLX',\n",
       " 'HD',\n",
       " 'HON',\n",
       " 'HRL',\n",
       " 'HST',\n",
       " 'HWM',\n",
       " 'HPQ',\n",
       " 'HUM',\n",
       " 'HBAN',\n",
       " 'HII',\n",
       " 'IEX',\n",
       " 'IDXX',\n",
       " 'INFO',\n",
       " 'ITW',\n",
       " 'ILMN',\n",
       " 'INCY',\n",
       " 'IR',\n",
       " 'INTC',\n",
       " 'ICE',\n",
       " 'IBM',\n",
       " 'IP',\n",
       " 'IPG',\n",
       " 'IFF',\n",
       " 'INTU',\n",
       " 'ISRG',\n",
       " 'IVZ',\n",
       " 'IPGP',\n",
       " 'IQV',\n",
       " 'IRM',\n",
       " 'JKHY',\n",
       " 'J',\n",
       " 'JBHT',\n",
       " 'SJM',\n",
       " 'JNJ',\n",
       " 'JCI',\n",
       " 'JPM',\n",
       " 'JNPR',\n",
       " 'KSU',\n",
       " 'K',\n",
       " 'KEY',\n",
       " 'KEYS',\n",
       " 'KMB',\n",
       " 'KIM',\n",
       " 'KMI',\n",
       " 'KLAC',\n",
       " 'KHC',\n",
       " 'KR',\n",
       " 'LB',\n",
       " 'LHX',\n",
       " 'LH',\n",
       " 'LRCX',\n",
       " 'LW',\n",
       " 'LVS',\n",
       " 'LEG',\n",
       " 'LDOS',\n",
       " 'LEN',\n",
       " 'LLY',\n",
       " 'LNC',\n",
       " 'LIN',\n",
       " 'LYV',\n",
       " 'LKQ',\n",
       " 'LMT',\n",
       " 'L',\n",
       " 'LOW',\n",
       " 'LUMN',\n",
       " 'LYB',\n",
       " 'MTB',\n",
       " 'MRO',\n",
       " 'MPC',\n",
       " 'MKTX',\n",
       " 'MAR',\n",
       " 'MMC',\n",
       " 'MLM',\n",
       " 'MAS',\n",
       " 'MA',\n",
       " 'MKC',\n",
       " 'MXIM',\n",
       " 'MCD',\n",
       " 'MCK',\n",
       " 'MDT',\n",
       " 'MRK',\n",
       " 'MET',\n",
       " 'MTD',\n",
       " 'MGM',\n",
       " 'MCHP',\n",
       " 'MU',\n",
       " 'MSFT',\n",
       " 'MAA',\n",
       " 'MHK',\n",
       " 'TAP',\n",
       " 'MDLZ',\n",
       " 'MPWR',\n",
       " 'MNST',\n",
       " 'MCO',\n",
       " 'MS',\n",
       " 'MOS',\n",
       " 'MSI',\n",
       " 'MSCI',\n",
       " 'NDAQ',\n",
       " 'NTAP',\n",
       " 'NFLX',\n",
       " 'NWL',\n",
       " 'NEM',\n",
       " 'NWSA',\n",
       " 'NWS',\n",
       " 'NEE',\n",
       " 'NLSN',\n",
       " 'NKE',\n",
       " 'NI',\n",
       " 'NSC',\n",
       " 'NTRS',\n",
       " 'NOC',\n",
       " 'NLOK',\n",
       " 'NCLH',\n",
       " 'NOV',\n",
       " 'NRG',\n",
       " 'NUE',\n",
       " 'NVDA',\n",
       " 'NVR',\n",
       " 'NXPI',\n",
       " 'ORLY',\n",
       " 'OXY',\n",
       " 'ODFL',\n",
       " 'OMC',\n",
       " 'OKE',\n",
       " 'ORCL',\n",
       " 'OTIS',\n",
       " 'PCAR',\n",
       " 'PKG',\n",
       " 'PH',\n",
       " 'PAYX',\n",
       " 'PAYC',\n",
       " 'PYPL',\n",
       " 'PENN',\n",
       " 'PNR',\n",
       " 'PBCT',\n",
       " 'PEP',\n",
       " 'PKI',\n",
       " 'PRGO',\n",
       " 'PFE',\n",
       " 'PM',\n",
       " 'PSX',\n",
       " 'PNW',\n",
       " 'PXD',\n",
       " 'PNC',\n",
       " 'POOL',\n",
       " 'PPG',\n",
       " 'PPL',\n",
       " 'PFG',\n",
       " 'PG',\n",
       " 'PGR',\n",
       " 'PLD',\n",
       " 'PRU',\n",
       " 'PEG',\n",
       " 'PSA',\n",
       " 'PHM',\n",
       " 'PVH',\n",
       " 'QRVO',\n",
       " 'PWR',\n",
       " 'QCOM',\n",
       " 'DGX',\n",
       " 'RL',\n",
       " 'RJF',\n",
       " 'RTX',\n",
       " 'O',\n",
       " 'REG',\n",
       " 'REGN',\n",
       " 'RF',\n",
       " 'RSG',\n",
       " 'RMD',\n",
       " 'RHI',\n",
       " 'ROK',\n",
       " 'ROL',\n",
       " 'ROP',\n",
       " 'ROST',\n",
       " 'RCL',\n",
       " 'SPGI',\n",
       " 'CRM',\n",
       " 'SBAC',\n",
       " 'SLB',\n",
       " 'STX',\n",
       " 'SEE',\n",
       " 'SRE',\n",
       " 'NOW',\n",
       " 'SHW',\n",
       " 'SPG',\n",
       " 'SWKS',\n",
       " 'SNA',\n",
       " 'SO',\n",
       " 'LUV',\n",
       " 'SWK',\n",
       " 'SBUX',\n",
       " 'STT',\n",
       " 'STE',\n",
       " 'SYK',\n",
       " 'SIVB',\n",
       " 'SYF',\n",
       " 'SNPS',\n",
       " 'SYY',\n",
       " 'TMUS',\n",
       " 'TROW',\n",
       " 'TTWO',\n",
       " 'TPR',\n",
       " 'TGT',\n",
       " 'TEL',\n",
       " 'TDY',\n",
       " 'TFX',\n",
       " 'TER',\n",
       " 'TSLA',\n",
       " 'TXN',\n",
       " 'TXT',\n",
       " 'TMO',\n",
       " 'TJX',\n",
       " 'TSCO',\n",
       " 'TT',\n",
       " 'TDG',\n",
       " 'TRV',\n",
       " 'TRMB',\n",
       " 'TFC',\n",
       " 'TWTR',\n",
       " 'TYL',\n",
       " 'TSN',\n",
       " 'UDR',\n",
       " 'ULTA',\n",
       " 'USB',\n",
       " 'UAA',\n",
       " 'UA',\n",
       " 'UNP',\n",
       " 'UAL',\n",
       " 'UNH',\n",
       " 'UPS',\n",
       " 'URI',\n",
       " 'UHS',\n",
       " 'UNM',\n",
       " 'VLO',\n",
       " 'VAR',\n",
       " 'VTR',\n",
       " 'VRSN',\n",
       " 'VRSK',\n",
       " 'VZ',\n",
       " 'VRTX',\n",
       " 'VFC',\n",
       " 'VIAC',\n",
       " 'VTRS',\n",
       " 'V',\n",
       " 'VNO',\n",
       " 'VMC',\n",
       " 'WRB',\n",
       " 'WAB',\n",
       " 'WMT',\n",
       " 'WBA',\n",
       " 'DIS',\n",
       " 'WM',\n",
       " 'WAT',\n",
       " 'WEC',\n",
       " 'WFC',\n",
       " 'WELL',\n",
       " 'WST',\n",
       " 'WDC',\n",
       " 'WU',\n",
       " 'WRK',\n",
       " 'WY',\n",
       " 'WHR',\n",
       " 'WMB',\n",
       " 'WLTW',\n",
       " 'WYNN',\n",
       " 'XEL',\n",
       " 'XLNX',\n",
       " 'XYL',\n",
       " 'YUM',\n",
       " 'ZBRA',\n",
       " 'ZBH',\n",
       " 'ZION',\n",
       " 'ZTS',\n",
       " '^GSPC']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_tickers = DM_YF.get_active_tickers()\n",
    "active_tickers_list = [t['tickerKey'] for t in active_tickers]\n",
    "#active_tickers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/tickers_config.json\n",
      "Download ANA.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: ANA.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/ANA.MC.TT.csv\n",
      "Download ACX.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: ACX.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/ACX.MC.TT.csv\n",
      "Download ACS.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: ACS.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/ACS.MC.TT.csv\n",
      "Root dir:  C:\\Users\\alvaro\\Repos\\Python_knowledge\\Appdatabase project\n",
      "parent_dir_path:  C:\\Users\\alvaro\\Repos\\Python_knowledge\\Appdatabase project\n",
      "dir_list:  ['Data', 'Tickers', 'Dummy1']\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "file path:  C:\\Users\\alvaro\\Repos\\Python_knowledge\\Appdatabase project/Data/Tickers/Dummy1/tickers_config.json\n"
     ]
    }
   ],
   "source": [
    "DM_YF.download_selected_tickers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download market ticker\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/tickers_config.json\n",
      "Download ANA.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: ANA.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/ANA.MC.TT.csv\n",
      "Download ACX.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: ACX.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/ACX.MC.TT.csv\n",
      "Download ACS.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: ACS.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/ACS.MC.TT.csv\n",
      "Download AENA.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: AENA.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/AENA.MC.TT.csv\n",
      "Download ALM.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: ALM.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/ALM.MC.TT.csv\n",
      "Download AMS.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: AMS.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/AMS.MC.TT.csv\n",
      "Download MTS.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: MTS.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/MTS.MC.TT.csv\n",
      "Download SAB.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: SAB.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/SAB.MC.TT.csv\n",
      "Download SAN.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: SAN.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/SAN.MC.TT.csv\n",
      "Download BKIA.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: BKIA.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/BKIA.MC.TT.csv\n",
      "Download BKT.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: BKT.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/BKT.MC.TT.csv\n",
      "Download BBVA.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: BBVA.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/BBVA.MC.TT.csv\n",
      "Download CABK.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: CABK.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/CABK.MC.TT.csv\n",
      "Download CLNX.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: CLNX.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/CLNX.MC.TT.csv\n",
      "Download CIE.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: CIE.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/CIE.MC.TT.csv\n",
      "Download ENG.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: ENG.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/ENG.MC.TT.csv\n",
      "Download ELE.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: ELE.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/ELE.MC.TT.csv\n",
      "Download FER.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: FER.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/FER.MC.TT.csv\n",
      "Download GRF.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: GRF.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/GRF.MC.TT.csv\n",
      "Download IAG.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: IAG.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/IAG.MC.TT.csv\n",
      "Download IBE.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: IBE.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/IBE.MC.TT.csv\n",
      "Download ITX.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: ITX.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/ITX.MC.TT.csv\n",
      "Download IDR.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: IDR.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/IDR.MC.TT.csv\n",
      "Download COL.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: COL.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/COL.MC.TT.csv\n",
      "Download MAP.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: MAP.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/MAP.MC.TT.csv\n",
      "Download MEL.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: MEL.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/MEL.MC.TT.csv\n",
      "Download MRL.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: MRL.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/MRL.MC.TT.csv\n",
      "Download NTGY.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: NTGY.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/NTGY.MC.TT.csv\n",
      "Download PHM.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: PHM.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/PHM.MC.TT.csv\n",
      "Download REE.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: REE.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/REE.MC.TT.csv\n",
      "Download REP.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: REP.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/REP.MC.TT.csv\n",
      "Download SGRE.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: SGRE.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/SGRE.MC.TT.csv\n",
      "Download SLR.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: SLR.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/SLR.MC.TT.csv\n",
      "Download TEF.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: TEF.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/TEF.MC.TT.csv\n",
      "Download VIS.MC.TT  from scratch\n",
      "Downloading from scratch historic data of: VIS.MC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/VIS.MC.TT.csv\n",
      "Download ^IBEX  from scratch\n",
      "Downloading from scratch historic data of: ^IBEX\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "./Data/Tickers/Dummy1/^IBEX.csv\n",
      "Root dir:  C:\\Users\\alvaro\\Repos\\Python_knowledge\\Appdatabase project\n",
      "parent_dir_path:  C:\\Users\\alvaro\\Repos\\Python_knowledge\\Appdatabase project\n",
      "dir_list:  ['Data', 'Tickers', 'Dummy1']\n",
      "nested_dir_path:  /Data/Tickers/Dummy1\n",
      "file path:  C:\\Users\\alvaro\\Repos\\Python_knowledge\\Appdatabase project/Data/Tickers/Dummy1/tickers_config.json\n"
     ]
    }
   ],
   "source": [
    "DM_YF.download_market_data('IBEX35', False) #last_date(True) / from scratch(False, default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = DM_YF.get_tickers()\n",
    "tickers[3]['data_update']\n",
    "#print(tickers[3]['data_update'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DM_YF.timeBounded_download('ACX.MC', 'ACX.MC.TT',\"03-23-2021\")\n",
    "DM_YF.timeBounded_download(tickers[4]['ticker'], tickers[4]['tickerKey'],tickers[4]['data_update'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DM_YF.download_ticker_data_from_last_date('ACX.MC', 'ACX.MC.TT',\"2021-03-24\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_date_definition(self):\n",
    "    return start_date\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date_str = a[0]['data_update']\n",
    "last_date = datetime.strptime(last_date_str, '%m-%d-%Y')\n",
    "print(last_date)\n",
    "start_date = last_date + timedelta(days=+1)\n",
    "print(start_date)\n",
    "print(start_date >= datetime.now())\n",
    "#start_date = None\n",
    "ticker = 'ANA.MC'\n",
    "def timeBounded_download(self, ticker, ticker_key, last_date):\n",
    "    # Se activa opción de descargar desde la última fecha \n",
    "    # 3 casos: A) start is None, B) start >= today C) start < today\n",
    "    delta = now-start_date\n",
    "    if start_date is None: # Aquí va un download_from_scratch\n",
    "        print(ticker + \" is not found in database, adding ----\")\n",
    "        #data_csv = yf.download(ticker) # Aquí va un download_from_scratch\n",
    "        self.download_ticker_data_from_scratch(ticker, ticker_key)\n",
    "    elif delta.days <= 0: # Aquí no hay download\n",
    "        print('Data is already updated')\n",
    "        return\n",
    "    else: # Aquí un download_from_last_date\n",
    "        #data_csv = yf.download(ticker, start = start_date)\n",
    "        # Función para calcular la start_date\n",
    "        start_date = self.start_date_calculation(last_date)\n",
    "        # Función download_ticker_data_from_last_date\n",
    "        self.download_ticker_data_from_last_date(ticker, ticker_key, start_date)\n",
    "        delta = now-start_date\n",
    "        print('Downloaded(days): ', delta.days)\n",
    "    return data_csv\n",
    "    \n",
    "    #return data_csv = yf.download(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = timeBounded_download('ACS')\n",
    "data_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date_str = a[0]['data_update']\n",
    "print(type(last_date))\n",
    "#last_date = pd.to_datetime(last_date, errors='coerce')\n",
    "last_date = datetime.strptime(last_date_str, '%m-%d-%Y')\n",
    "#print(last_date)\n",
    "start_date = last_date + timedelta(days=-2)\n",
    "#start_date = pd.to_datetime(start_date, errors='coerce')\n",
    "#start_date = None\n",
    "print(last_date)\n",
    "print(start_date), print(type(start_date))\n",
    "today = date.today()\n",
    "now = datetime.now()\n",
    "#print(today), print(type(today))\n",
    "print(now), print(type(now))\n",
    "#print(today.strftime(\"%d-%m-%Y\"))\n",
    "print(start_date >= now)\n",
    "delta = start_date - now\n",
    "print(delta.days)\n",
    "#data_csv = yf.download('ANA.MC', start = start_date)\n",
    "#data_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = DM_YF.get_active_tickers()\n",
    "a[0]['data_update'], a[2]['data_update']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data_csv = yf.download('ANA.MC')\n",
    "last_date = a[0]['data_update']\n",
    "last_date = pd.to_datetime(last_date, errors='coerce')\n",
    "last_date.date()\n",
    "start_date = last_date + timedelta(days=1)\n",
    "start_date_1 = last_date + timedelta(days=-2)\n",
    "last_date, start_date, start_date_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = yf.download('ANA.MC', start=start_date_1, end=datetime.now())\n",
    "#data_csv = yf.download('ANA.MC')\n",
    "data_csv.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date = a[0]['data_update'] \n",
    "last_date = pd.to_datetime(last_date, errors='coerce')\n",
    "last_date = last_date + timedelta(days=-2)\n",
    "last_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start = last_date\n",
    "start = None\n",
    "end = end=datetime.now()\n",
    "def fun1(start = start, end = end):\n",
    "    if start is not None:\n",
    "        data_csv = yf.download('ANA.MC',start,  end)\n",
    "    else:\n",
    "        data_csv = yf.download('ANA.MC')\n",
    "    return data_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = fun1()\n",
    "type(data_csv)\n",
    "data_csv.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_dict = {'start':last_date}\n",
    "def fun2(**kwargs):\n",
    "    data_csv = yf.download('ANA.MC', **kwargs)\n",
    "        #data_csv = yf.download('ANA.MC',start,  end)\n",
    "    return data_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = fun2()\n",
    "type(data_csv)\n",
    "data_csv.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_dict = {'start':last_date}\n",
    "data_csv = yf.download('ANA.MC', **date_dict)\n",
    "data_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = yf.download('ANA.MC', end=datetime.now())\n",
    "#data_csv = yf.download('ANA.MC')\n",
    "data_csv.head(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitectura descarga tickers\n",
    "fdsa\n",
    "fdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(a = 1):\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1(424)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers[0]['feeds']['ticker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ticker = DM_YF.download_ticker_data(tickers[0]['feeds']['ticker'], tickers[0]['tickerKey'])\n",
    "#data_ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ticker.head(), type(data_ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DM_YF.download_market_data('IBEX35')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markets = DM_YF.get_markets()\n",
    "markets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DM_YF.download_all_markets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Last_Data(ticker, market, backup):\n",
    "    # Check if the market is in the database. In case that not exist, WARNING and return empty csv.\n",
    "    markets = os.listdir('./Data/')\n",
    "    if market not in markets:\n",
    "        print(\"There is no market in database, please add new market and fill with data\")\n",
    "        data_csv = []\n",
    "        return data_csv\n",
    "\n",
    "    # If market exists between the markets loaded, then go to market folder and load shares list\n",
    "    # Get shares of a market from the folder that it is stored\n",
    "    shares = os.listdir('./Data/' + market)\n",
    "    shares = [s.replace('.csv', '') for s in shares]\n",
    "\n",
    "    # Function to download full historic data, used for backup and in firts time download\n",
    "    def dowload_historic_data(ticker):\n",
    "        print('Downloading historic data of: ' + ticker)\n",
    "        data_csv = yf.download(ticker)\n",
    "        data_csv.insert(loc=0, column='Date', value=pd.to_datetime(data_csv.index, errors='coerce'))\n",
    "        data_csv['Date'] = [time.date() for time in data_csv['Date']]\n",
    "        data_csv.reset_index(drop=True, inplace=True)\n",
    "        return data_csv\n",
    "\n",
    "    if ticker in shares:  # IN CASE THAT THE SHARE EXISTS PREVIOUSLY. If share in LISTA_ACCIONES\n",
    "        if backup:  # BACK UP CASE. Download data from the historic in YahooFinance\n",
    "            data_csv = dowload_historic_data(ticker)\n",
    "        else:  # NO BACK UP CASE. Download data from the last date updated\n",
    "            # Load de data\n",
    "            print(\"-------------------------------------------\")\n",
    "            print(ticker + \" exists in Database. \" + \"Opening \" + ticker)\n",
    "            data_csv = pd.read_csv(\"./Data/\" + market + \"/\" + ticker + \".csv\")\n",
    "            # Read and get the date of the last day\n",
    "            last_date = data_csv[\"Date\"].iloc[-1]\n",
    "            last_date = pd.to_datetime(last_date, errors='coerce')\n",
    "            last_date.date()\n",
    "            # If last date is today, then return the file with no modifications\n",
    "            if last_date >= date.today():\n",
    "                print(\"File is already updated. No modifications.\")\n",
    "                # print (\"-------------------------------------------\")\n",
    "                return data_csv\n",
    "            # print(\"Updating \" + ticker +' from '+ last_date +' until '+date.today()+'(today)' )\n",
    "            print('Updating ' + ticker + ' until today')\n",
    "            start_date = last_date + timedelta(days=1)\n",
    "            # Download data from the selected date\n",
    "            data = yf.download(ticker, start=start_date, end=datetime.now())\n",
    "            data.insert(loc=0, column='Date', value=pd.to_datetime(data.index, errors='coerce'))\n",
    "            data['Date'] = [time.date() for time in data['Date']]\n",
    "            data.reset_index(drop=True, inplace=True)\n",
    "            data_csv = data_csv.append(data, ignore_index=True)\n",
    "    else:  # IN CASE THAT THE SHARE DOESNT EXISTS PREVIOUSLY. Download historic data.\n",
    "        print(ticker + \" is not found in database, adding ----\")\n",
    "        data_csv = dowload_historic_data(ticker)\n",
    "    return data_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_ticker_data_from_last_date(ticker, ticker_key, start_date):\n",
    "        print('Updating historic data of: ' + ticker)\n",
    "        # 1. Descargar datos desde la ultima fecha\n",
    "        data_csv = yf.download(ticker, start = start_date)\n",
    "        data_csv.insert(loc=0, column='Date', value=pd.to_datetime(data_csv.index, errors='coerce'))\n",
    "        data_csv['Date'] = [time.date() for time in data_csv['Date']]\n",
    "        # 2. Cargar el csv\n",
    "        #data_csv_local = DM_YF.load_ticker_data(ticker_key)\n",
    "        data_csv_local = pd.read_csv('./Data/Tickers/Dummy1/ACX.MC.TT.csv')\n",
    "        # 3. Apendear los datos que faltan, resetear el index y esta será la nueva varaible data_csv\n",
    "        print(type(data_csv_local)),print(type(data_csv))\n",
    "        data_csv = pd.concat([data_csv_local, data_csv], ignore_index = True)\n",
    "        data_csv.reset_index(drop=True, inplace=True)\n",
    "        data_csv.drop(data_csv.columns[0], axis = 1, inplace = True)\n",
    "        #data_csv.drop(1)\n",
    "        # 4. Guardar los datos\n",
    "        #self.save_ticker_data(ticker_key,data_csv )\n",
    "        return data_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_ticker_data_from_last_date('ANA.MC', 'ANA.MC.TT',\"2021-03-23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = DM_YF.get_tickers()\n",
    "a[0]['data_update'], a[9]['data_update']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date = a[0]['data_update']\n",
    "print(type(last_date))\n",
    "#last_date = pd.to_datetime(last_date, errors='coerce')\n",
    "last_date = datetime.strptime(last_date, '%m-%d-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DM_YF.load_ticker_data('ACX.MC.TT')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('./Data/Tickers/Dummy1/ACX.MC.TT.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_market_data(self, market):\n",
    "    print('Download market ticker')\n",
    "    tickers = self.get_active_tickers()\n",
    "    [DM_YF.download_ticker_data(t['feeds']['ticker'], t['tickerKey']) for t in tickers if t['market'] in markets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = DM_YF.get_active_tickers() # Get active tickers\n",
    "#tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markets = ['IBEX35']\n",
    "[print(t) for t in tickers if t['market'] in markets]\n",
    "\n",
    "DM_YF.download_ticker_data(tickers[0]['feeds']['ticker'], tickers[0]['tickerKey'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[DM_YF.download_ticker_data(t['feeds']['ticker'], t['tickerKey']) for t in tickers if t['market'] in markets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import housekeeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = ['Data']\n",
    "file_name = 'tickers_config.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myHousekeeper = housekeeper.instance_class() \n",
    "data = myHousekeeper.load_json_to_list(dir_list, file_name)\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markets = []\n",
    "active_tickers = []\n",
    "for d in data:\n",
    "    markets.append(d['market'])\n",
    "    if d['active_type']=='stock' and d['active_flag']:\n",
    "        active_tickers.append(d)\n",
    "    elif d['active_type']=='ETF':\n",
    "        active_tickers.append(d)\n",
    "markets = set(markets)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markets, active_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0:2]['market']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyValList = ['a']\n",
    "data[0]['market'] in keyValList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data[0]['active_flag']:\n",
    "    print('hola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    ">>> exampleSet = [{'type':'type1'},{'type':'type2'},{'type':'type2'}, {'type':'type3'}]\n",
    ">>> keyValList = ['type2','type3']\n",
    ">>> expectedResult = [d for d in exampleSet if d['type'] in keyValList]\n",
    ">>> expectedResult\n",
    "[{'type': 'type2'}, {'type': 'type2'}, {'type': 'type3'}]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    ">>> list(filter(lambda d: d['type'] in keyValList, exampleSet))\n",
    "[{'type': 'type2'}, {'type': 'type2'}, {'type': 'type3'}]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.finxter.com/how-to-filter-a-list-of-dictionaries-in-python/#Where_to_Go_From_Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DM_YF.get_tickers_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DM_YF.load_tickers_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = ['a', 'b', 'c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list.pop(0)\n",
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list.pop(0)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
